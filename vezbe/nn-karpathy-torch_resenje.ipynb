{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Veštačke neuronske mreže\n",
    "Zasnovano na:\\\n",
    "https://github.com/karpathy/micrograd \\\n",
    "https://www.youtube.com/watch?v=VMj-3S1tku0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "    \n",
    "    def __init__(self, data, _children=()):\n",
    "        \"\"\"\n",
    "        data (number) -> Vrednost čvora u expression grafu\n",
    "        grad -> Vrednost lokalnog gradijenta, koristi se za backprop\n",
    "        _children -> Transformiše se u _prev, predstavlja čvorove koji prethode tretutnom -> potrebno za backprop\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self._prev = set(_children) # tuple koji treba da predstavi prethodne čvorove u expression grafu\n",
    "        self.grad = 0.0 # Inicijalno ga postavljamo na 0 -> nema efekat na izlaz\n",
    "        self._backward = lambda:None # funkcija koja je zaduzena za primenu pravila ulančavanja, po defaultu prazna funkcija -> za listove\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data}, grad={self.grad})\"\n",
    "    \n",
    "    # Overload operatora\n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other) # kako bismo podrzali a + 1\n",
    "        out = Value(self.data + other.data, (self, other))\n",
    "        # Prosirenje svakog \"cvora\" u grafu\n",
    "        # Funkcija koja propagira gradijent\n",
    "        def _backward():\n",
    "            # CLOSURE\n",
    "            # kod sabiranja samo propagiramo gradijent\n",
    "            self.grad += 1.0 * out.grad # MORA +=, ZAŠTO -> odgovor b = a + a\n",
    "            other.grad += 1.0 * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __radd__(self, other):\n",
    "        return self + other\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        # ZADATAK 1: dovrši implementaciju\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data * other.data, (self, other))\n",
    "        def _backward():\n",
    "            self.grad += other.data * out.grad\n",
    "            other.grad += self.data * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __rmul__(self, other):\n",
    "        # Jer Python ne zna eksplicitno da je a * 2 isto što i 2 * a\n",
    "        return self * other\n",
    "    # Za potrebe aktivacione funkcije (npr. TANH) moramo da overloadujemo dodatne operatore\n",
    "    # jer nam + i * nisu dovoljni\n",
    "    # Sa druge strane, nije neophodno da dodjemo do atomičnog nivoa\n",
    "    # Funkcije koje pravimo mogu biti proizvoljno kompleksne\n",
    "    # DOKLE GOD MOZEMO DA IZRAČUNAMO LOKALNI IZVOD\n",
    "    def tanh(self):\n",
    "        x = self.data\n",
    "        t = (math.exp(2*x) - 1)/(math.exp(2*x) + 1)\n",
    "        def _backward():\n",
    "            # ZADATAK 2: dovrši implementaciju\n",
    "            self.grad = (1 - t ** 2) * out.grad\n",
    "        out = Value(t, (self, ))\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def exp(self):\n",
    "        x = self.data\n",
    "        out = Value(math.exp(x), (self, ))\n",
    "    \n",
    "        def _backward():\n",
    "            self.grad += out.data * out.grad\n",
    "        out._backward = _backward()\n",
    "        return out\n",
    "    \n",
    "    def __pow__(self, other):\n",
    "        assert isinstance(other, (int, float)) # only support int/float powers\n",
    "        out = Value(self.data ** other, (self, ))\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += other * (self.data ** (other - 1)) * out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __truediv__(self, other):\n",
    "        # a / b\n",
    "        # a * (1 / b)\n",
    "        # a * b ^ -1\n",
    "        # stepen ima prioritet u odnosu na *\n",
    "        return self * other ** -1\n",
    "    \n",
    "    def __neg__(self):\n",
    "        return self * -1\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        return self + (-other)\n",
    "    \n",
    "    def backward(self):\n",
    "        topo = []\n",
    "        visited = set()\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "        build_topo(self)\n",
    "\n",
    "        self.grad = 1.0\n",
    "        # Backpropagation je rekurzivna primena pravila ulančavanja kroz graf\n",
    "        for node in reversed(topo):\n",
    "            node._backward()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Topološko sortiranje grafa](img/topo.png)\n",
    "U nastavku su implementirane apstrakcije koje nam olakšavaju kreiranje neuronske mreže. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "\n",
    "    # Prilikom kreiranja neurona, svakom ulazu se pridružuje težina\n",
    "    # I dodajemo bias\n",
    "    def __init__(self, nin):\n",
    "        self.w = [Value(random.uniform(-1, 1)) for _ in range(nin)]\n",
    "        self.b = Value(random.uniform(-1, 1))\n",
    "\n",
    "    def __call__(self, x):\n",
    "        act = self.b\n",
    "        for wi, xi in zip(self.w, x):\n",
    "            act += wi * xi\n",
    "        out = act.tanh()\n",
    "        return out\n",
    "\n",
    "    def params(self):\n",
    "        return self.w + [self.b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    # Broj ulaza u neuron je jednak broju neurona iz prethodnog sloja\n",
    "    def __init__(self, nin, nout):\n",
    "        self.neurons = [Neuron(nin) for _ in range(nout)]\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        outs = [n(x) for n in self.neurons]\n",
    "        return outs\n",
    "    \n",
    "    def params(self):\n",
    "        params = []\n",
    "        for neuron in self.neurons:\n",
    "            params.extend(neuron.params())\n",
    "        return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, nin, nouts):\n",
    "        sz = [nin] + nouts\n",
    "        self.layers = [Layer(sz[i], sz[i + 1]) for i in range(len(nouts))]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "    def params(self):\n",
    "        return [p for layer in self.layers for p in layer.params()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Value(data=0.8504102225958426, grad=0.0)],\n",
       " [Value(data=0.9333648547303176, grad=0.0)],\n",
       " [Value(data=0.7943337900980459, grad=0.0)],\n",
       " [Value(data=0.9338881229295876, grad=0.0)]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs = [\n",
    "    [2.0, 3.0, -1.0],\n",
    "    [3.0, -1.0, 0.5],\n",
    "    [0.5, 1.0, 1.0],\n",
    "    [1.0, 1.0, -1.0]\n",
    "]\n",
    "\n",
    "n = MLP(3, [4, 4, 1])\n",
    "ys = [1.0, -1.0, -1.0, 1.0] # desired targets\n",
    "ypred=[n(x) for x in xs]\n",
    "ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 6.9842812935876\n",
      "1 6.869183365184368\n",
      "2 6.731553962562069\n",
      "3 6.5654192273229555\n",
      "4 6.362983626153945\n",
      "5 6.114251500768667\n",
      "6 5.807062854611538\n",
      "7 5.428531385478896\n",
      "8 4.969755834354813\n",
      "9 4.4354270682815935\n",
      "10 3.8549507179853455\n",
      "11 3.2819912067456687\n",
      "12 2.7715336357226774\n",
      "13 2.3512813752629933\n",
      "14 2.0173557677805913\n",
      "15 1.7515096601155584\n",
      "16 1.5360172332388067\n",
      "17 1.3581721439580177\n",
      "18 1.2094777708951843\n",
      "19 1.0840371002206726\n",
      "20 0.977481552565535\n",
      "21 0.8864132258355424\n",
      "22 0.8081218593016767\n",
      "23 0.7404227686772584\n",
      "24 0.6815470716924266\n",
      "25 0.6300584035336211\n",
      "26 0.5847868018107812\n",
      "27 0.544775736804733\n",
      "28 0.5092397891744491\n",
      "29 0.4775309801898987\n",
      "30 0.44911205733192283\n",
      "31 0.4235353105640892\n",
      "32 0.40042575395288477\n",
      "33 0.3794677394919611\n",
      "34 0.36039426645027056\n",
      "35 0.34297840952125136\n",
      "36 0.3270264161487755\n",
      "37 0.31237212292699074\n",
      "38 0.298872418263157\n",
      "39 0.28640353828428744\n",
      "40 0.27485802916977864\n",
      "41 0.26414224481815496\n",
      "42 0.2541742764356235\n",
      "43 0.2448822321390009\n",
      "44 0.23620280142831968\n",
      "45 0.22808005249833774\n",
      "46 0.22046442065712377\n",
      "47 0.21331185424025523\n",
      "48 0.20658309083764387\n",
      "49 0.20024304175971766\n",
      "50 0.19426026674755315\n",
      "51 0.18860652419892998\n",
      "52 0.1832563848105831\n",
      "53 0.17818689865931986\n",
      "54 0.17337730746506902\n",
      "55 0.16880879517863165\n",
      "56 0.16446427117985635\n",
      "57 0.16032818130856183\n",
      "58 0.1563863427206896\n",
      "59 0.15262579919762517\n",
      "60 0.1490346940626352\n",
      "61 0.14560215829521822\n",
      "62 0.1423182117980556\n",
      "63 0.13917367607533562\n",
      "64 0.13616009683604804\n",
      "65 0.13326967525002917\n",
      "66 0.13049520676505846\n",
      "67 0.1278300265458689\n",
      "68 0.12526796072521762\n",
      "69 0.1228032827670024\n",
      "70 0.12043067433495155\n",
      "71 0.11814519014030213\n",
      "72 0.11594222631023399\n",
      "73 0.1138174918774778\n",
      "74 0.11176698304193741\n",
      "75 0.10978695989861684\n",
      "76 0.10787392536366147\n",
      "77 0.10602460606279887\n",
      "78 0.10423593497461256\n",
      "79 0.1025050356455525\n",
      "80 0.10082920781488225\n",
      "81 0.0992059143063338\n",
      "82 0.09763276905947899\n",
      "83 0.09610752618803159\n",
      "84 0.09462806996475741\n",
      "85 0.09319240564361302\n",
      "86 0.0917986510393677\n",
      "87 0.09044502879345183\n",
      "88 0.08912985926226694\n",
      "89 0.08785155397082114\n",
      "90 0.08660860958041879\n",
      "91 0.08539960232433402\n",
      "92 0.08422318287002725\n",
      "93 0.08307807157055935\n",
      "94 0.08196305407153395\n",
      "95 0.08087697724314968\n",
      "96 0.07981874540986791\n",
      "97 0.07878731685280262\n",
      "98 0.07778170056227583\n",
      "99 0.07680095322007466\n",
      "100 0.07584417639281901\n",
      "101 0.07491051391954394\n",
      "102 0.0739991494781106\n",
      "103 0.07310930431643346\n",
      "104 0.07224023513574213\n",
      "105 0.07139123211420612\n",
      "106 0.07056161706026307\n",
      "107 0.06975074168589505\n",
      "108 0.06895798599092334\n",
      "109 0.06818275675014215\n",
      "110 0.06742448609578282\n",
      "111 0.06668263018842306\n",
      "112 0.06595666797001068\n",
      "113 0.06524609999318001\n",
      "114 0.06455044732150973\n",
      "115 0.06386925049578315\n",
      "116 0.06320206856170946\n",
      "117 0.06254847815490737\n",
      "118 0.061908072639280484\n",
      "119 0.06128046129520689\n",
      "120 0.060665268554234646\n",
      "121 0.060062133277222506\n",
      "122 0.059470708073093376\n",
      "123 0.05889065865557351\n",
      "124 0.05832166323548444\n",
      "125 0.05776341194632906\n",
      "126 0.057215606301072745\n",
      "127 0.05667795867817601\n",
      "128 0.05615019183506195\n",
      "129 0.055632038447339716\n",
      "130 0.05512324067221122\n",
      "131 0.05462354973460388\n",
      "132 0.05413272553466732\n",
      "133 0.05365053627536492\n",
      "134 0.053176758108976346\n",
      "135 0.05271117480140678\n",
      "136 0.05225357741326823\n",
      "137 0.05180376399677068\n",
      "138 0.051361539307518904\n",
      "139 0.05092671453037171\n",
      "140 0.05049910701857267\n",
      "141 0.050078540045414094\n",
      "142 0.049664842567738225\n",
      "143 0.049257849000627574\n",
      "144 0.04885739900267344\n",
      "145 0.048463337271250996\n",
      "146 0.04807551334726317\n",
      "147 0.047693781428848586\n",
      "148 0.047318000193579755\n",
      "149 0.04694803262870438\n",
      "150 0.04658374586901195\n",
      "151 0.04622501104192914\n",
      "152 0.045871703119473556\n",
      "153 0.04552370077671471\n",
      "154 0.04518088625641303\n",
      "155 0.04484314523952622\n",
      "156 0.04451036672128812\n",
      "157 0.04418244289258563\n",
      "158 0.04385926902636967\n",
      "159 0.04354074336885669\n",
      "160 0.04322676703528535\n",
      "161 0.04291724391000929\n",
      "162 0.04261208055071713\n",
      "163 0.042311186096584415\n",
      "164 0.04201447218016914\n",
      "165 0.04172185284287663\n",
      "166 0.041433244453825166\n",
      "167 0.04114856563195608\n",
      "168 0.04086773717123733\n",
      "169 0.04059068196881862\n",
      "170 0.040317324956004805\n",
      "171 0.04004759303191844\n",
      "172 0.03978141499973163\n",
      "173 0.039518721505351226\n",
      "174 0.03925944497844953\n",
      "175 0.03900351957573538\n",
      "176 0.03875088112636921\n",
      "177 0.038501467079426296\n",
      "178 0.03825521645332089\n",
      "179 0.038012069787106736\n",
      "180 0.03777196909357229\n",
      "181 0.03753485781405516\n",
      "182 0.03730068077490327\n",
      "183 0.037069384145512425\n",
      "184 0.036840915397874444\n",
      "185 0.036615223267574104\n",
      "186 0.03639225771617364\n",
      "187 0.03617196989492791\n",
      "188 0.035954312109777016\n",
      "189 0.0357392377875627\n",
      "190 0.03552670144342011\n",
      "191 0.03531665864929727\n",
      "192 0.035109066003557544\n",
      "193 0.034903881101621334\n",
      "194 0.03470106250760644\n",
      "195 0.03450056972692813\n",
      "196 0.0343023631798201\n",
      "197 0.03410640417574161\n",
      "198 0.033912654888636824\n",
      "199 0.03372107833301203\n",
      "200 0.033531638340800446\n",
      "201 0.03334429953898531\n",
      "202 0.033159027327950716\n",
      "203 0.03297578786053428\n",
      "204 0.03279454802175456\n",
      "205 0.03261527540918913\n",
      "206 0.03243793831397679\n",
      "207 0.03226250570242425\n",
      "208 0.03208894719819185\n",
      "209 0.03191723306503966\n",
      "210 0.031747334190112095\n",
      "211 0.031579222067741816\n",
      "212 0.03141286878375499\n",
      "213 0.031248247000259147\n",
      "214 0.031085329940896467\n",
      "215 0.030924091376547128\n",
      "216 0.030764505611464772\n",
      "217 0.0306065474698311\n",
      "218 0.030450192282713742\n",
      "219 0.030295415875413262\n",
      "220 0.030142194555186793\n",
      "221 0.029990505099334302\n",
      "222 0.029840324743635904\n",
      "223 0.02969163117112752\n",
      "224 0.02954440250120397\n",
      "225 0.029398617279038385\n",
      "226 0.029254254465306755\n",
      "227 0.029111293426208367\n",
      "228 0.02896971392377104\n",
      "229 0.028829496106433095\n",
      "230 0.028690620499891864\n",
      "231 0.028553067998210443\n",
      "232 0.02841681985517417\n",
      "233 0.028281857675888797\n",
      "234 0.02814816340861242\n",
      "235 0.028015719336813495\n",
      "236 0.027884508071448023\n",
      "237 0.02775451254344876\n",
      "238 0.027625715996419616\n",
      "239 0.027498101979528528\n",
      "240 0.027371654340593327\n",
      "241 0.027246357219353742\n",
      "242 0.027122195040923995\n",
      "243 0.026999152509420414\n",
      "244 0.026877214601758966\n",
      "245 0.02675636656161653\n",
      "246 0.026636593893551972\n",
      "247 0.0265178823572813\n",
      "248 0.026400217962103033\n",
      "249 0.02628358696146813\n"
     ]
    }
   ],
   "source": [
    "# Trening neuronske mreže\n",
    "num_epochs = 250\n",
    "learning_rate = 0.01\n",
    "for k in range(num_epochs):\n",
    "    # forward pass\n",
    "    ypred = [n(x) for x in xs]\n",
    "    loss = sum(((yout[0] - ygt) ** 2 for ygt, yout in zip(ys, ypred)), Value(0.0))\n",
    "\n",
    "    # zero grad\n",
    "    for p in n.params():\n",
    "        p.grad = 0.0\n",
    "\n",
    "    # backward pass\n",
    "    # Za svaki cvor u grafu računamo izvod u odnosu na L\n",
    "    loss.backward()\n",
    "\n",
    "    # update\n",
    "    for p in n.params():\n",
    "        p.data += -learning_rate * p.grad\n",
    "    \n",
    "    print(k, loss.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Value(data=0.9143671591342561, grad=0.0)],\n",
       " [Value(data=-0.9390700920114902, grad=0.0)],\n",
       " [Value(data=-0.8937088569622151, grad=0.0)],\n",
       " [Value(data=0.938155585238551, grad=0.0)]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ypred=[n(x) for x in xs]\n",
    "ypred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch\n",
    "\n",
    "PyTorch je open-source biblioteka za mašinsko učenje.\n",
    "\n",
    "Koristićemo je kako bismo jednostavno kreirali i trenirali neuronske mreže.\n",
    "\n",
    "Pored osnovnih stvari PyTorch nam pruža velik broj opcija za optimizaciju, vizualizaciju...\n",
    "\n",
    "Dosadašnji kod smo koristili kako bismo razumeli kako funkcioniše trening neuronske mreže. U nastavku ćemo koristiti PyTorch da kreiramo isti model.\n",
    "\n",
    "Tensor je osnovna jedinica podataka u PyTorch-u (višedimenzionalni nizovi). U dosadašnjoj implementaciji ta osnovna jedinica je bila skalar.\n",
    "\n",
    "\"Matematika ostaje potpuno identična, tensori se uvode isključivo zbog efikasnosti.\" - <i>Andrej Karpathy</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/250], Loss: 4.1201\n",
      "Epoch [51/250], Loss: 0.1673\n",
      "Epoch [101/250], Loss: 0.0544\n",
      "Epoch [151/250], Loss: 0.0302\n",
      "Epoch [201/250], Loss: 0.0204\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.9527],\n",
       "        [-0.9462],\n",
       "        [-0.9279],\n",
       "        [ 0.9298]], grad_fn=<TanhBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Kreiranje 2 potpuno ekvivalentna modela pomoću PyTorch\n",
    "model1 = nn.Sequential(\n",
    "    nn.Linear(3, 4),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(4, 4),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(4, 1),\n",
    "    nn.Tanh()\n",
    ")\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 4)\n",
    "        self.fc2 = nn.Linear(4, 4)\n",
    "        self.fc3 = nn.Linear(4, 1)\n",
    "        self.tanh = nn.Tanh()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.tanh(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.tanh(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.tanh(out)\n",
    "        return out\n",
    "\n",
    "model2 = MLP(3)\n",
    "\n",
    "\n",
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomLoss, self).__init__()\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        loss = torch.tensor(0.0)\n",
    "        for y_gt, y_out in zip(y_true, y_pred):\n",
    "            loss += torch.sum((y_out[0] - y_gt) ** 2)\n",
    "        return loss\n",
    "\n",
    "num_epochs = 250\n",
    "loss_fn = CustomLoss()\n",
    "optimizer = optim.SGD(model1.parameters(), lr=0.01)\n",
    "\n",
    "X = torch.tensor(xs)\n",
    "y = torch.tensor(ys)\n",
    "\n",
    "for n in range(num_epochs):\n",
    "    y_pred = model1(X)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    \n",
    "    if n % 50 == 0:\n",
    "        print(f'Epoch [{n+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "y_pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ori-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
